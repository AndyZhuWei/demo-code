#记录线上问题
##线上CPU变高
###第一个
 1.使用top命令查询CPU使用率较高的进程，加入是7443
  2.ps p 7443 -L -o pcpu,pid,tid,time,tname,cmd 通过这个命令查询此进程中使用率较高的线程tid(十进制)，比如是7453
  3.printf "%x\n" 7453 转换成十六进制 是 0x1d1d
  4.通过jstack -l 7443命令查询线程 dump信息，输出如下信息：
 
  "Thread-0" #8 prio=5 os_prio=0 tid=0x00007f052c0f0800 nid=0x1d1d runnable [0x00007f051b339000]
     java.lang.Thread.State: RUNNABLE
  	at java.io.FileOutputStream.writeBytes(Native Method)
  	at java.io.FileOutputStream.write(FileOutputStream.java:326)
  	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
  	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
  	- locked <0x00000000f5d69a68> (a java.io.BufferedOutputStream)
  	at java.io.PrintStream.write(PrintStream.java:482)
  	- locked <0x00000000f5d60cc0> (a java.io.PrintStream)
  	at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)
  	at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)
  	at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104)
  	- locked <0x00000000f5d69aa8> (a java.io.OutputStreamWriter)
  	at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185)
  	at java.io.PrintStream.newLine(PrintStream.java:546)
  	- eliminated <0x00000000f5d60cc0> (a java.io.PrintStream)
  	at java.io.PrintStream.println(PrintStream.java:807)
  	- locked <0x00000000f5d60cc0> (a java.io.PrintStream)
  	at com.example.demo.thread.FindJavaThreadInTaskManager$Worker.run(FindJavaThreadInTaskManager.java:20)
  	at java.lang.Thread.run(Thread.java:748)
 
  	通过对比我们就知道了问题代码是在倒出第二行
###第二个
线上的定时任务每个一个固定的时间点CPU和内存占比都比较高
1.使用top，发现使用较高的线程号是20957
2.top -H -p 20957 查询改进程号下哪些线程使用率较高，发现有4个线程较高
3.分别执行printf "%x\n" 20969、printf "%x\n" 20970、printf "%x\n" 20959和printf "%x\n" 20978
4.将上边得到的十六进制信息带入到线程堆栈中查找，线程堆栈通过jstack 20957 |grep tid -A 30
5.分析
  通过观察发现这些线程都是JVM的gc线程。此时可以基本确定是内存不足或内存泄漏导致gc线程持续运行，导致CPU占用过高
6.内存定位
 使用jstat -gcutil 20957 2000 10 观察gc日志信息
 ```
 S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT   
  0.00   0.00 100.00  99.99  26.31     42   21.917   218 1484.830 1506.747
  0.00   0.00 100.00  99.99  26.31     42   21.917   218 1484.830 1506.747
  0.00   0.00 100.00  99.99  26.31     42   21.917   219 1496.567 1518.484
  0.00   0.00 100.00  99.99  26.31     42   21.917   219 1496.567 1518.484
  0.00   0.00 100.00  99.99  26.31     42   21.917   219 1496.567 1518.484
  0.00   0.00 100.00  99.99  26.31     42   21.917   219 1496.567 1518.484
  0.00   0.00 100.00  99.99  26.31     42   21.917   219 1496.567 1518.484
  0.00   0.00 100.00  99.99  26.31     42   21.917   220 1505.439 1527.355
  0.00   0.00 100.00  99.99  26.31     42   21.917   220 1505.439 1527.355
  0.00   0.00 100.00  99.99  26.31     42   21.917   220 1505.439 1527.355
```
  	从输出信息可以看出，Eden区内存占用100%，Old区内存占用99.99%，Full GC的次数高达220次，并且频繁Full GC，Full GC的持续时间也特别长，
  	平均每次Full GC耗时6.8秒（1505.439/220）。根据这些信息，基本可以确定是程序代码上出现了问题，可能存在不合理创建对象的地方
分析堆栈
再堆栈信息中查找带有项目目录并且线程状态是RUNABLE的相关信息，最后发现是由于有一个定时任务查询出了大量的数据导致的。怎么导致的呢？
根据条件查询时，有一个字段open_id信息为空，则导致了动态sql语句过滤了这个条件直接查询了全表数据，有大概90w的数据,加载的线程一直再加载。导致JVM一直GC


##误操作导致大批量数据错误更新
背景：
需要更新珊瑚数据库中卡激活表中48条记录的产品code和产品名称，在dbeaver软件中执行了以下语句：
```sql
update card_activation t set t.product_name ='四川国寿遂宁快捷垫付卡',t.product_id ='I20BQ1'
where card_num in(
'666601332117',
'666601332118',
'666601332119',
'666601332120',
'666601332146',
'666601332147',
'666601332148',
'666601332151',
'666601332152',
'666601332156',
'666601332157',
'666601332164',
'666601332165',
'666601332172',
'666601332173',
'666601332176',
'666601332177',
'666601332178',
'666601332183',
'666601332184',
'666601332188',
'666601332189',
'666601332193',
'666601332194',
'666601332199',
'666601332200',
'666601332203',
'666601332210',
'666601332214',
'666601332218',
'666601332221',
'666601332222',
'666601332223',
'666601332229',
'666601332246',
'666601332306',
'666601332310',
'666601332315',
'666601332316',
'666601332321',
'666601332326',
'666601332327',
'666601332336',
'666601332337',
'666601332339',
'666601332360',
'666601332361',
'666601332450'
)
```
在这个软件中，没有软件手动提交，在执行时只是单击第一行语句前边的开始按钮，并没有选择所有sql语句的情况执行。导致更新了全表47w
的数据，而且时自动提交了。此时手有点颤

解决过程：
1.首先想到回复，但是生产库中实时有数据进来，所以放弃。
2.让运维帮忙把前一天的备份库恢复到另一个实例中，通过两个实例库中的表进行关联更新。但是
两个实例库如何用sql关联起来不清楚，当时查资料时间耽误不起，前方有什么坑也不清楚，所以放弃。
进而考虑拷贝相应的表到目标库，在做表之间的关联更新，但是拷贝速度实在太慢，就放弃了。
3.通过导出激活表中的pmid，在导入到有产品信息的实例库中，做关联查询，最后在导入到激活表所在库做关联更新，但是导入导出速度太慢，放弃。
4.编写线上代码，做远程查询，查询出信息后，在一条一条更新激活表产品信息，更新速度太慢，放弃
5.在4的基础上把每条更新的操作，改为在另一个表中做批量插入，等所有数据批量插入完成后，在和激活表做关联更新，批量插入速度很快，可行
关联更新语句也很快，47w的数据不到一分钟就更新完成
```sql
update card_activation ca inner join pmid_info_temp b on ca.pmid =b.pmid 
set ca.product_id =b.product_code ,ca.product_name =b.product_name 
```
总结：
后边操作要细心，对数据更改的操作一定要设置手动提交
能线上进行的操作尽量通过线上功能解决。
对解决问题的过程中，发现的技术短板进行补充，比如如何快速进行数据迁移，有什么好的工具。备份文件的恢复细则等等



##数据库的一张表保存报错 

背景
之前在项目A中保存操作挺正常，突然同事负责的业务（也在项目A）出现保存异常，调用的方法都是一致的，报错信息如下：
org.springframework.dao.DuplicateKeyException: 
### Error updating database.  Cause: java.sql.SQLIntegrityConstraintViolationException: ORA-00001: unique constraint (WPT.PRI_RECORD_SHUIDI_ID) violated
查询我的保存业务也出现了同样的问题。

刚看到就是数据库主键冲突，按照这个方向去查找的数据库主键生成的方式，在项目中使用了mybatis框架，保存调用方法如下
```sql
 <insert id="insert" parameterType="cn.healthlink.shuidi.model.ShuiDiRecordData" >
    <selectKey keyColumn="id" keyProperty="id" resultType="String" order="BEFORE">
      select xxx.Nextval from dual
    </selectKey>
    insert into xxxx (id, supplier_no, sd_supplier_name, order_no, policy_no, status, product_no,
                name, mobile, id_type, id_code, effect_time, invalid_time, create_time,SYN_CRM_STATUS,SYN_CRM_MESSAGE,
                channel,OUT_USER_ID, CHANNEL_CODE, SEX,WAIT_PERIOD,PACKAGE_CODE,IS_INSTALMENT,SCHEME_CODE,SCHEME_NAME,PMID)
    values (#{id,jdbcType=VARCHAR}, #{supplierNo,jdbcType=VARCHAR}, #{sdSupplierName,jdbcType=VARCHAR},
      #{orderNo,jdbcType=VARCHAR}, #{policyNo,jdbcType=VARCHAR}, #{status,jdbcType=VARCHAR},
      #{productNo,jdbcType=VARCHAR}, #{name,jdbcType=VARCHAR}, #{mobile,jdbcType=VARCHAR},
      #{idType,jdbcType=VARCHAR}, #{idCode,jdbcType=VARCHAR},#{effectTime,jdbcType=DATE},
       #{invalidTime,jdbcType=DATE},#{createTime,jdbcType=DATE},#{synCrmStatus,jdbcType=VARCHAR},#{synCrmMessage,jdbcType=VARCHAR},
       #{channel,jdbcType=VARCHAR},#{outUserId,jdbcType=VARCHAR}, #{channelCode,jdbcType=VARCHAR},#{sex,jdbcType=VARCHAR},
       #{waitPeriod,jdbcType=VARCHAR}, #{packageCode,jdbcType=VARCHAR},#{isInstalment,jdbcType=VARCHAR},
    #{schemeCode,jdbcType=VARCHAR},#{schemeName,jdbcType=VARCHAR},#{pmid,jdbcType=VARCHAR})
  </insert>
```
主键的生成都是通过oracle数据库中的序列方式，按理应该没错，开始环境是不是因为在高并发场景下<selectKey>获取主键的方式有问题，经过在本地测试，没有发现冲突的地方。
查看日志出现大概都是几种在晚上1点到5点之间的操作。怀疑是不是定时任务有问题，但查询*本项目*的定时任务发现并不是，开始怀疑在这个时间段客户的调用量大导致并发的问题。
同事提醒说 是不是上次优化导致的（此处的优化就是把保存操作由同步改为异步）。最后找不到问题，就将信将疑的把同步改为异步，外加了sql日志输出的配置
```
mybatis:
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
```
等第二天再来看日志
第二天发现问题依旧，但是发现了插入数据冲突时的id值，根据这个id值取数据库查询，发现了一条陌生的数据，根据这条数据的渠道号，查询发现这条数据来自另一个项目，看到希望，在另一个
定时人的项目中发现保存这个表的数据库主键居然是这样的配置的
```sql
  */
    @Insert({"insert into xx (id,policy_no,status,product_no,name,mobile,id_type,id_code,effect_time,invalid_time,create_time,channel,syn_crm_status,syn_crm_message) values" +
            " (#{id},#{policyNo},#{status},#{productNo},#{name},#{mobile},#{idType},#{idCode},#{effectTime},#{invalidTime},#{createTime},#{channel},#{synCrmStatus},#{synCrmMessage})"})
    @SelectKey(statement = "select max(id+1) id from xx",
            keyProperty = "id",
            resultType = Long.class,
            before = true)
    int insert(xx recordShuidi);
```
而且这个业务对应的定时任务时间就是从1点开始，此次发现了问题

总结：其实开始时思路是对的，但就是局限在了这个项目本身，没有想到其他项目可能导致的这个问题



##迷惑的字符串

调试时明明看着字符串的值和待比较的值时一样的，程序判断就是不相等。很是困惑
其实表面看是一样的，其实他们的字符构成中可能会有隐藏的一些字符信息，调试时看一单看看字符数组信息，就可以看出来


##AES加解密报错
报错信息如下：
Input length must be multiple of 16 when decrypting with padded cipher
误区: 误以为所有的字节数组都可以new String(),然后在通过String.getBytes()还原

解决的办法:

可以用base64对参生的数组进行编码,然后在解码,这样不会像new String(byte[]),getBytes()那样造成数组前后不一致,一开始,
我看到大部分人都是用base64,我也只是以为多一层编码看起来安全一些而已,没想到base64对数组的处理是不会造成误差的

就是直接返回数组,然后再用数组解密咯






















