# 安装
准备4台服务器，安装一个完全分布式的。
（node01:192.168.80.200）
（node02:192.168.80.202）
（node03:192.168.80.204）
（node04:192.168.80.206）
保证每台机器上都安装了java 最低1.7
（安装在了/opt/jdk1.8）
先在一个机器上安装，然后在进行分发
步骤：
1.下载
wget http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz
2.解压
tar -xf  zookeeper-3.4.6.arg.gz
解压过后，一般我们要放在/opt/下
mkdir /opt/zhuwei
mv zookeeper-3.4.6 /opt/
进入zookeeper-3.4.6看一下目录结构
（注意：不要用yum安装jdk,yum安装的jdk不是oracle的hotspot的jdk，
  而是openjdk.好多开源软件早期还对openjdk进行测试后续只对oracle的jdk进行测试，所以可能由兼容性问题）
3.配置文件
拷贝配置
cp zoo_sample.cfg zoo.cfg
zookeeper默认启动加载的配置文件时zoo.cfg
```
tickTime=2000   #leader于follower,和客户端之间的心跳时间默认2秒
initLimit=10    #leader可以容忍等待follower于它的初始延迟时间10*tickTime
syncLimit=5    #leader于follower同步时时限5*tickTime
dataDir=/tmp/zookeeper   #zookeeper持久化的目录，最好不要用tmp这样的目录，修改成/var/zhuwei/zk,以后存储日志、快照等数据文件
clientPort=2181   #客户端连接zookeeper服务进程时的端口
#maxClientCnxns=60 #当前启动的进程服务允许连接的最大客户端数量
#需要手动填写都有哪些zookeeper的服务节点，不像redis那样不用。
#这些服务节点的数量除以2加1就是过半的节点数
##3888表示的是当服务器刚启动还没有leader的时候活着leader挂掉的时候其余节点要在这个端口号建立socket进行通信选择新的leader
#当选举出leader后，leader会启动一个2888的端口，其他节点在leader的2888的端口进行连接，然后后续的在由leader节点的情况下创建节点什么的事务都是
#在2888端口上进行通信的
#leader其实不是选举出来的 而是推让出来的，一般会在过半的数量中选择最大的数字，所以
#1 2 3 4表示的就是代表他们的数字
server.1=node01:2888:3888  
server.2=node02:2888:3888
server.3=node03:2888:3888
server.4=node04:2888:3888
```
保存
4.创建目录
/var/zk
后边这个目录会存方一些持久化数据。
除此之外我们还需要在这个目录中创建一个文件myid,文件中的内容就是我们在上述配置文件中配置的server.1中的1。
其他机器就是相应的2、3、4等信息
echo 1 > myid

5.配置环境变量
在/etc/profile中配置zookeeper的环境变量
export ZOOKEEPER_HOME=/opt/zookeeper-3.4.6
export PATH=$PATH:$ZOOKEEPER_HOME/bin

6.启动
zkServer.sh start-foreground
（如果和其他节点连接异常可能就是防火墙端口没有关闭）
7.查看启动状态
zkServer.sh status
8.客户端连接
zkCli.sh 默认连接自己
连接后输入help可以看到一些常用的命令


###基本操作
因为我们说zookeeper是基于目录树结构的，所以有一个ls path显示目录的命令
ls /  #显示跟目录
```
[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper]
[zk: localhost:2181(CONNECTED) 1] help
ZooKeeper -server host:port cmd args
	stat path [watch]
	set path data [version]
	ls path [watch]
	delquota [-n|-b] path
	ls2 path [watch]
	setAcl path acl
	setquota -n|-b val path
	history 
	redo cmdno
	printwatches on|off
	delete path [version]
	sync path
	listquota path
	rmr path
	get path [watch]
	create [-s] [-e] path data acl
	addauth scheme auth
	quit 
	getAcl path
	close 
	connect host:port
[zk: localhost:2181(CONNECTED) 2] create /ooxx   #如果在创建目录时不带数据也要加“”不然会创建不成功
[zk: localhost:2181(CONNECTED) 3] ls /
[zookeeper]
[zk: localhost:2181(CONNECTED) 4] create /ooxx ""
Created /ooxx
[zk: localhost:2181(CONNECTED) 5] ls /
[zookeeper, ooxx]
[zk: localhost:2181(CONNECTED) 6] create /ooxx/xxoo "" #还可以创建子目录
Created /ooxx/xxoo
[zk: localhost:2181(CONNECTED) 7] ls /ooxx
[xxoo]
[zk: localhost:2181(CONNECTED) 8] get /ooxx   #创建目录时可以存入数据，当要取数据时通过get
""                       #存入的数据
cZxid = 0x100000004
ctime = Sat Nov 28 09:12:32 CST 2020
mZxid = 0x100000004
mtime = Sat Nov 28 09:12:32 CST 2020
pZxid = 0x100000005
cversion = 1
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 2
numChildren = 1
[zk: localhost:2181(CONNECTED) 9] set /ooxx "hello"     #还可以通过set设置目录上的数据
cZxid = 0x100000004
ctime = Sat Nov 28 09:12:32 CST 2020
mZxid = 0x100000006
mtime = Sat Nov 28 09:16:52 CST 2020
pZxid = 0x100000005
cversion = 1
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 7
numChildren = 1
[zk: localhost:2181(CONNECTED) 10] get /ooxx   #得到目录上的数据
"hello"             #这个数据只能放1M,它也是二进制安全的（客户端推什么二进制数组我就存什么，不管编解码的东西）
cZxid = 0x100000004
ctime = Sat Nov 28 09:12:32 CST 2020
mZxid = 0x100000006
mtime = Sat Nov 28 09:16:52 CST 2020
pZxid = 0x100000005
cversion = 1
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 7
numChildren = 1
[zk: localhost:2181(CONNECTED) 11] get /ooxx/xxoo
""
cZxid = 0x100000005
ctime = Sat Nov 28 09:14:15 CST 2020
mZxid = 0x100000005
mtime = Sat Nov 28 09:14:15 CST 2020
pZxid = 0x100000005
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 2
numChildren = 0
[zk: localhost:2181(CONNECTED) 12]
```
说明信息：
**cZxid**
cZxid = 0x100000004（创建的事务id）
其中c表示create
id表示的就是leader维护的递增的事务id，提交给zookeeper的命令时严格有序的，这是通过zookeeper中leader保证的，
   leader是一个单机，单机维护一个递增的序列是什么容易的，这个id值是由64位组成的，其中低32位就是递增的序列。也就是
   0x100000004中的00000004
   0x100000004中高32位省略了，其中1表示的是leader开启的基元，如果有切换过leader，这个1就会被新leader递增1变为2
**ctime**
创建时间
**mZxid**
修改的事务id
mZxid = 0x100000006
此处可以看出创建事务id和修改事务id之间少了一个0x100000005，这个事务id其实是创建目录/ooxx/xxoo的，我们可以从上述命令中
get /ooxx/xxoo 得到验证。这就可以说明在zookeeper中这些操作都是严格按照顺序的
**mtime**
修改事务的时间
**pZxid**
这个表示当前节点下最后创建的节点id号
**ephemeralOwner**
表示当前节点没有归属，也就是持久节点，退出后在登陆上去还是可以看到整个节点
ephemeralOwner = 0x0
在创建节点的时候是可以选择创建临时节点的选项
当我们连接一个新的客户端上来的时候，会对每个客户端创建一个sessionId（0x2760c643b320000）,如下：
```
2020-11-28 09:09:59,707 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /127.0.0.1:53681
2020-11-28 09:09:59,753 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /127.0.0.1:53681
2020-11-28 09:09:59,809 [myid:2] - INFO  [CommitProcessor:2:ZooKeeperServer@617] - Established session 0x2760c643b320000 with negotiated timeout 30000 for client /127.0.0.1:53681
```
我们用整个客户端创建一个节点
create -e /oxox "asfasfdasf"
```
[zk: localhost:2181(CONNECTED) 12] create -e /oxox "asfasfdasf"
Created /oxox
[zk: localhost:2181(CONNECTED) 13] get /oxox
"asfasfdasf"
cZxid = 0x100000007
ctime = Sat Nov 28 09:45:04 CST 2020
mZxid = 0x100000007
mtime = Sat Nov 28 09:45:04 CST 2020
pZxid = 0x100000007
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x2760c643b320000    #此时可以看到这个信息就是sessionId了
dataLength = 12
numChildren = 0
[zk: localhost:2181(CONNECTED) 14]
```
这个选项就代表了这个节点归属于这个sessionId,只要这个客户端断开，这个节点就会消失
这个就是临时节点，伴随着session的会话期
思考一个问题：
如果我连接的那个服务节点挂掉了，那么我这个session创建的临时节点会消失嘛？
首先告诉答案，不会消失，首先服务端不光会同步各节点的数据，还会同步各个客户端连接过来来的sessionId,即统一视图不光数据是统一的，sessionID也会统一
我们这么来验证一下：
我们在node04节点上创建一个节点
```
[zk: localhost:2181(CONNECTED) 6] get /zhuwei
"adfadf"
cZxid = 0x100000012
ctime = Sat Nov 28 10:17:33 CST 2020
mZxid = 0x100000012
mtime = Sat Nov 28 10:17:33 CST 2020
pZxid = 0x100000012
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0
[zk: localhost:2181(CONNECTED) 7]
```
然后我们在node02启动一个客户端
最后我们在node04上在创建一个节点
```
[zk: localhost:2181(CONNECTED) 7] create /andy "adfadf"
Created /andy
[zk: localhost:2181(CONNECTED) 8] get /andy
"adfadf"
cZxid = 0x100000014
ctime = Sat Nov 28 10:18:16 CST 2020
mZxid = 0x100000014
mtime = Sat Nov 28 10:18:16 CST 2020
pZxid = 0x100000014
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0
[zk: localhost:2181(CONNECTED) 9] 
```
发现在node04上两次创建节点的事务id中间差了一个0x100000013，其实这个id就是node02连接上来创建的sessionId需要通过leader同步给所有的服务节点消耗的。
所以当某个服务节点挂掉后，我们客户端缓存的sessionId会转移到其他服务节点上，所以在超时时间内转移过来后，那个sessionId所创建的临时节点是不会消失的
客户端断开后也会进行相应的同步，统一视图



### 序列节点
我们多个客户端向同一个目录树下创建时可能冲突，我们可以使用如下命令避免
create -s /xxx ""  #-s表示序列
在node02上执行
```
[zk: localhost:2181(CONNECTED) 0] create -s /abc "fadsfasdf"
Created /abc0000000006         #自动在目录后追加了一些序号信息

```
在node04上执行
```
[zk: localhost:2181(CONNECTED) 9] create -s /abc "fadsfasfsafdfdf"
Created /abc0000000007         #可以看到递增的效果,不会覆盖之前相同的目录

```
这样可以对分布式系统做隔离，做统一命名。
创建后会返回相应的节点名称


###应用场景
zookeeper提供的功能
* 统一配置管理
  通过1M的数据可以完成
* 分组管理
  通过path结构也就是分层的命名空间
* 统一命名
  靠sequential支撑
* 同步
  临时节点
  
####场景一
通过以上几点可以实现：
* 分布式锁
  依靠临时节点
  
再升级
* 队列式事务的锁
锁依托一个父节点且具备-s代表父节点可以拥有多把锁
  
以上讲的这些场景都需要客户端代码实现，zookeeper只提供最简单的特性，不同的人
通过客户端的组成可以实现各种功能
  
####场景二
HA 选主


###小结
不要把zookeeper当作数据库，查询是可以的，少写东西。只有那些金贵的需要协调的再
写数据，剩下的通过其被动感知。只读就可以了







###演示4台集群中服务节点杂技2888和3888之间通信的过程
再4台机器上执行
netstat -natp | egrep '(2888|3888)'


3888：选主投票用的
2888：leader接收请求

### zookeeper案例
分布式协调主要协调什么呢？？？
#### 分布式配置注册发现
需要把配置数据存储到zk中，其他客户端get+watch
主要：再new zk的时候可以把连接字符串后加一个目录，这个zk就会只工作再找个目录下了
代码见zookeeper-demo项目


#### 分布式锁
1.争抢锁，只有一个人能获得锁
2.获得锁的人出问题，（临时节点），防止自己挂了产生死锁
3.获得锁的人成功了，释放锁
4.锁释放或被删除了别人怎么知道
 4-1：主动轮询，心跳。。。弊端：延迟
 4-2：watch：解决延迟问题，，弊端：通信上有压力。大量的通知监听者产品大量监听
 4-3：sequence+watch:wacher谁？watch前一个，最小的获得锁！一旦，最小的释放了锁，成本：zk只给第二个发事件回调！！！
 
重入锁


代码见zookeeper-demo项目




















  
  
